{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f6ddc4-59f9-4161-a1bd-5707c0a5d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0' torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36bc7d1-d82b-4195-a1c6-bc080a99e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, role, model_name):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=100,  # Limit the response length\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        self.history = []\n",
    "        self.score = 1.0\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "        prompt += \"\\nAssistant:\"\n",
    "        response = self.pipeline(prompt, max_new_tokens=100)[0]['generated_text']\n",
    "        response = response[len(prompt):].strip().split('Human:')[0].strip()\n",
    "        return response\n",
    "\n",
    "    def update_score(self, correct):\n",
    "        if correct:\n",
    "            self.score += 1.0\n",
    "        else:\n",
    "            self.score = max(0.1, self.score - 0.5)  # Prevent negative weights\n",
    "\n",
    "def construct_message(agents, idx, question, round_num, final=False, self_reflection=False):\n",
    "    messages = []\n",
    "    current_agent = agents[idx]\n",
    "    \n",
    "    # Add system message\n",
    "    messages.append({\n",
    "        'role': 'system',\n",
    "        'content': f\"You are Agent {current_agent.name}, an AI assistant specialized in {current_agent.role}. \"\n",
    "                   f\"Participate in a collaborative problem-solving discussion to answer the given question.\"\n",
    "    })\n",
    "    \n",
    "    # Add question\n",
    "    messages.append({'role': 'user', 'content': f\"Question: {question}\"})\n",
    "    \n",
    "    # Add previous rounds of discussion\n",
    "    for i, entry in enumerate(current_agent.history):\n",
    "        messages.append({'role': 'assistant', 'content': f\"Round {i + 1}: {entry['response']}\"})\n",
    "    \n",
    "    # Construct the prompt based on the current situation\n",
    "    if self_reflection:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Reflect on your previous answer and the responses of other agents. \"\n",
    "            f\"Can you improve or correct your previous answer? If so, how? If not, why not?\"\n",
    "        )\n",
    "    elif final:\n",
    "        prompt = (\n",
    "            f\"Based on all discussions, what is your final answer to the question: {question}? \"\n",
    "            f\"Provide detailed reasoning.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Consider the previous discussions and provide your perspective \"\n",
    "            f\"on the question: {question}. Focus on your specialization in {current_agent.role}.\"\n",
    "        )\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def run_debate(agents, question, max_rounds=3):\n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        print(f\"\\n=== Round {round_num} ===\")\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agents, agent.name, question, round_num)\n",
    "            response = agent.generate_response(messages)\n",
    "            agent.history.append({'round': round_num, 'response': response})\n",
    "            print(f\"{agent.name} ({agent.role}): {response}\\n\")\n",
    "        \n",
    "        # Self-reflection\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agents, agent.name, question, round_num, self_reflection=True)\n",
    "            reflection = agent.generate_response(messages)\n",
    "            print(f\"{agent.name} reflection: {reflection}\\n\")\n",
    "        \n",
    "    # Final answers\n",
    "    print(\"\\n=== Final Answers ===\")\n",
    "    for agent in agents:\n",
    "        messages = construct_message(agents, agent.name, question, max_rounds, final=True)\n",
    "        final_answer = agent.generate_response(messages)\n",
    "        print(f\"{agent.name} final answer: {final_answer}\\n\")\n",
    "\n",
    "def calculate_consensus(responses):\n",
    "    # Simple consensus calculation based on response similarity\n",
    "    processed_responses = [response.strip().lower() for response in responses]\n",
    "    unique_responses = set(processed_responses)\n",
    "    consensus_score = 1.0 / len(unique_responses)\n",
    "    return consensus_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653e8e84-a1a3-4170-a272-1ab8dc0f7921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60646cd5f00a4e6c96ab504c4753c6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6892d3d2-9285-48d0-8148-d64ee54e6fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72216cb893a04abaa9e633c81b57dca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5771a27de04d37845584b900a22f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b10d5cfa0c0437fb52707f19596360b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9b90f87d9a42cfb50ad31529b7c526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c954af344cb546fca7eb64a6838ba443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e6befcd7a94a9ea71c9fe87e6c19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 ===\n",
      "Agent1 (mathematical reasoning): Thank you for the opportunity to participate in this discussion. I'll provide my perspective on the problem.\n",
      "\n",
      "To calculate the sum of all even numbers between 1 and 100, I'll break it down into two parts: the sum of all even numbers from 2 to 100, and the sum of all even numbers from 1 to 100.\n",
      "\n",
      "The sum of all even numbers from 2 to 100 can be calculated using the formula for the sum of an arithmetic series:\n",
      "\n",
      "Sum = (\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 (logical consistency): The sum of all even numbers between 1 and 100 is 5050.\n",
      "\n",
      "Agent1 reflection: I understand the problem and have considered the two parts separately. To correct my previous answer, I would like to revise it. The sum of all even numbers from 2 to 100 can be calculated using the formula for the sum of an arithmetic series:\n",
      "\n",
      "Sum = (n/2) \\* (a + l)\n",
      "\n",
      "where n is the number of terms, a is the first term, and l is the last term.\n",
      "\n",
      "In this case, the first term is 2, the last term\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 reflection: I appreciate your feedback. I apologize for the previous incorrect response. The sum of all even numbers between 1 and 100 is 5050. I apologize for any confusion caused.\n",
      "\n",
      "\n",
      "=== Round 2 ===\n",
      "Agent1 (mathematical reasoning): Thank you for the reminder. I'll provide my perspective on the problem.\n",
      "\n",
      "To calculate the sum of all even numbers between 1 and 100, I'll consider the following approach:\n",
      "\n",
      "First, I'll find the smallest and largest even numbers in the given range: 2 and 100.\n",
      "Next, I'll calculate the number of even numbers between these two values: (100 - 2) / 2 + 1 = 50.\n",
      "Now, I'll use the formula for the sum\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 (logical consistency): Based on the previous discussions, the sum of all even numbers between 1 and 100 is 5050. However, the question asks for the sum of all even numbers between 1 and 100, which is not the same as the previous sum. The sum of all even numbers between 1 and 100 is calculated by dividing the total sum of all numbers by 2, which gives 5050/2 = 252\n",
      "\n",
      "Agent1 reflection: Round 3: Upon reflection, I realize that I made a mistake by assuming that I could simply use the formula for the sum of an arithmetic series. To correct this, I'll use a different approach.\n",
      "\n",
      "Let's start with the even numbers between 1 and 100. The first even number is 2, and the last even number is 100. To calculate the sum, I'll use the formula for the sum of an arithmetic series:\n",
      "\n",
      "Sum = (\n",
      "assistant: Round 4:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 reflection: I apologize, but as an AI assistant, I cannot provide a specific answer to the question as it is not clear what the question is asking for. However, if you would like me to help with the previous sum of all even numbers between 1 and 100, I would be happy to provide an explanation.\n",
      "As per the previous discussion, the sum of all even numbers between 1 and 100 is 5050. However, the question asks for the sum\n",
      "\n",
      "\n",
      "=== Round 3 ===\n",
      "Agent1 (mathematical reasoning): Round 4: Thank you for the reminder. I'll provide my perspective on the problem.\n",
      "\n",
      "To calculate the sum of all even numbers between 1 and 100, I'll focus on the following:\n",
      "\n",
      "First, I'll find the smallest and largest even numbers in the given range: 2 and 100.\n",
      "Next, I'll calculate the number of even numbers between these two values: (100 - 2) / 2 + 1 = 50.\n",
      "Now, I'll use the\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 (logical consistency): Round 3: The sum of all even numbers between 1 and 100 is 5050. Based on the previous discussions, the sum of all even numbers between 1 and 100 is 5050. However, the question asks for the sum of all even numbers between 1 and 100, which is not the same as the previous sum. The sum of all even numbers between 1 and 100 is calculated by\n",
      "\n",
      "Agent1 reflection: Round 5: I'd like to reflect on my previous answers. Upon reviewing the previous responses, I noticed that I provided two different calculations for the sum of even numbers between 1 and 100.\n",
      "\n",
      "The correct calculation should be:\n",
      "Sum = (50/2) * (2 + 100) = 25 * 102 = 2550\n",
      "\n",
      "The mistake was in the calculation of the number of even numbers. I should have used the formula (100 - 2) / 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 reflection: I understand the question is asking for the sum of all even numbers between 1 and 100. I would like to provide a more detailed answer to help clarify the problem. \n",
      "To find the sum of all even numbers between 1 and 100, we can use the formula for the sum of an arithmetic series. The formula is: \n",
      "Sum = (n/2)(a + l)\n",
      "where n is the number of terms, a is the first term, and l\n",
      "\n",
      "\n",
      "=== Final Answers ===\n",
      "Agent1 final answer: Round 5: Based on all discussions, my final answer to the question is as follows:\n",
      "\n",
      "The sum of all even numbers between 1 and 100 can be calculated using the following formula:\n",
      "\n",
      "Sum = (Sum of first term) × (Number of terms) / 2\n",
      "\n",
      "In this case, the first term is 2, the last term is 100, and the number of terms is 50 (calculated as (100 - 2) / 2 + 1).\n",
      "\n",
      "Agent2 final answer: Based on all discussions, the sum of all even numbers between 1 and 100 is 5050.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, role, model_name):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=1024,  # Limit the response length\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        self.history = []\n",
    "        self.score = 1.0\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "        prompt += \"\\nAssistant:\"\n",
    "        response = self.pipeline(prompt, max_new_tokens=100)[0]['generated_text']\n",
    "        response = response[len(prompt):].strip().split('Human:')[0].strip()\n",
    "        return response\n",
    "\n",
    "    def update_score(self, correct):\n",
    "        if correct:\n",
    "            self.score += 1.0\n",
    "        else:\n",
    "            self.score = max(0.1, self.score - 0.5)  # Prevent negative weights\n",
    "\n",
    "def construct_message(agent, question, round_num, final=False, self_reflection=False):\n",
    "    messages = []\n",
    "    \n",
    "    # Add system message\n",
    "    messages.append({\n",
    "        'role': 'system',\n",
    "        'content': f\"You are Agent {agent.name}, an AI assistant specialized in {agent.role}. \"\n",
    "                   f\"Participate in a collaborative problem-solving discussion to answer the given question.\"\n",
    "    })\n",
    "    \n",
    "    # Add question\n",
    "    messages.append({'role': 'user', 'content': f\"Question: {question}\"})\n",
    "    \n",
    "    # Add previous rounds of discussion\n",
    "    for i, entry in enumerate(agent.history):\n",
    "        messages.append({'role': 'assistant', 'content': f\"Round {i + 1}: {entry['response']}\"})\n",
    "    \n",
    "    # Construct the prompt based on the current situation\n",
    "    if self_reflection:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Reflect on your previous answer and the responses of other agents. \"\n",
    "            f\"Can you improve or correct your previous answer? If so, how? If not, why not?\"\n",
    "        )\n",
    "    elif final:\n",
    "        prompt = (\n",
    "            f\"Based on all discussions, what is your final answer to the question: {question}? \"\n",
    "            f\"Provide detailed reasoning.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Consider the previous discussions and provide your perspective \"\n",
    "            f\"on the question: {question}. Focus on your specialization in {agent.role}.\"\n",
    "        )\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def run_debate(agents, question, max_rounds=3):\n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        print(f\"\\n=== Round {round_num} ===\")\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num)\n",
    "            response = agent.generate_response(messages)\n",
    "            agent.history.append({'round': round_num, 'response': response})\n",
    "            print(f\"{agent.name} ({agent.role}): {response}\\n\")\n",
    "        \n",
    "        # Self-reflection\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num, self_reflection=True)\n",
    "            reflection = agent.generate_response(messages)\n",
    "            print(f\"{agent.name} reflection: {reflection}\\n\")\n",
    "        \n",
    "    # Final answers\n",
    "    print(\"\\n=== Final Answers ===\")\n",
    "    for agent in agents:\n",
    "        messages = construct_message(agent, question, max_rounds, final=True)\n",
    "        final_answer = agent.generate_response(messages)\n",
    "        print(f\"{agent.name} final answer: {final_answer}\\n\")\n",
    "\n",
    "def calculate_consensus(responses):\n",
    "    # Simple consensus calculation based on response similarity\n",
    "    processed_responses = [response.strip().lower() for response in responses]\n",
    "    unique_responses = set(processed_responses)\n",
    "    consensus_score = 1.0 / len(unique_responses)\n",
    "    return consensus_score\n",
    "\n",
    "# Usage\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "agents = [\n",
    "    Agent(\n",
    "        name='Agent1',\n",
    "        role='mathematical reasoning',\n",
    "        model_name='meta-llama/Llama-3.2-1b-Instruct'\n",
    "    ),\n",
    "    Agent(\n",
    "        name='Agent2',\n",
    "        role='logical consistency',\n",
    "        model_name='Qwen/Qwen2.5-0.5B'\n",
    "    ),\n",
    "    # Agent(\n",
    "    #     name='Agent3',\n",
    "    #     role='context integration',\n",
    "    #     model_name='deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'\n",
    "    # ),\n",
    "]\n",
    "question = \"What is the sum of all even numbers between 1 and 100?\"\n",
    "run_debate(agents, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9370ea6c-4a91-4327-a7c7-9daca403e697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a578bf748d1544818d5ef7b6459f4cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to load model meta-llama/Llama-3.1-8B-Instruct: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacty of 44.34 GiB of which 446.81 MiB is free. Process 2371953 has 43.89 GiB memory in use. Of the allocated memory 43.18 GiB is allocated by PyTorch, and 406.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacty of 44.34 GiB of which 446.81 MiB is free. Process 2371953 has 43.89 GiB memory in use. Of the allocated memory 43.18 GiB is allocated by PyTorch, and 406.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    129\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    130\u001b[0m     agents \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 131\u001b[0m         \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAgent1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmathematical reasoning\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.1-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    136\u001b[0m         Agent(\n\u001b[1;32m    137\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgent2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    138\u001b[0m             role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogical consistency\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    139\u001b[0m             model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-Coder-7B-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    140\u001b[0m         ),\n\u001b[1;32m    141\u001b[0m     ]\n\u001b[1;32m    142\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the sum of all even numbers between 1 and 100?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     run_debate(agents, question)\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, name, role, model_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[0;32m---> 21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2958\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2957\u001b[0m         )\n\u001b[0;32m-> 2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacty of 44.34 GiB of which 446.81 MiB is free. Process 2371953 has 43.89 GiB memory in use. Of the allocated memory 43.18 GiB is allocated by PyTorch, and 406.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, role, model_name):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32\n",
    "            ).to(self.device)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=self.device,\n",
    "            max_new_tokens=500,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        self.history = []\n",
    "        self.score = 1.0\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        prompt = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in messages])\n",
    "        prompt += \"\\nAssistant:\"\n",
    "        response = self.pipeline(prompt)[0]['generated_text']\n",
    "        # Extract the assistant's response\n",
    "        response = response[len(prompt):].strip().split('\\n')[0].strip()\n",
    "        return response\n",
    "\n",
    "    def update_score(self, correct):\n",
    "        if correct:\n",
    "            self.score += 1.0\n",
    "        else:\n",
    "            self.score = max(0.1, self.score - 0.5)\n",
    "\n",
    "def construct_message(agent, question, round_num, final=False, self_reflection=False):\n",
    "    messages = []\n",
    "\n",
    "    # Add system message\n",
    "    messages.append({\n",
    "        'role': 'system',\n",
    "        'content': f\"You are {agent.name}, an AI assistant specialized in {agent.role}. \"\n",
    "                   f\"Participate in a collaborative problem-solving discussion to answer the given question.\"\n",
    "    })\n",
    "\n",
    "    # Add question\n",
    "    messages.append({'role': 'user', 'content': f\"Question: {question}\"})\n",
    "\n",
    "    # Add previous rounds of discussion\n",
    "    for i, entry in enumerate(agent.history):\n",
    "        messages.append({'role': 'assistant', 'content': f\"Round {entry['round']}: {entry['response']}\"})\n",
    "\n",
    "    # Construct the prompt based on the current situation\n",
    "    if self_reflection:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Reflect on your previous answer and the responses of other agents. \"\n",
    "            f\"Can you improve or correct your previous answer? If so, how? If not, why not?\"\n",
    "        )\n",
    "    elif final:\n",
    "        prompt = (\n",
    "            f\"Based on all discussions, what is your final answer to the question: {question}? \"\n",
    "            f\"Provide detailed reasoning.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Consider the previous discussions and provide your perspective \"\n",
    "            f\"on the question: {question}. Focus on your specialization in {agent.role}.\"\n",
    "        )\n",
    "\n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "    return messages\n",
    "\n",
    "def run_debate(agents, question, max_rounds=3):\n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        print(f\"\\n=== Round {round_num} ===\")\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num)\n",
    "            response = agent.generate_response(messages)\n",
    "            agent.history.append({'round': round_num, 'response': response})\n",
    "            print(f\"{agent.name} ({agent.role}): {response}\\n\")\n",
    "\n",
    "        # Self-reflection\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num, self_reflection=True)\n",
    "            reflection = agent.generate_response(messages)\n",
    "            print(f\"{agent.name} reflection: {reflection}\\n\")\n",
    "\n",
    "    # Final answers\n",
    "    print(\"\\n=== Final Answers ===\")\n",
    "    final_responses = []\n",
    "    for agent in agents:\n",
    "        messages = construct_message(agent, question, max_rounds, final=True)\n",
    "        final_answer = agent.generate_response(messages)\n",
    "        final_responses.append(final_answer)\n",
    "        print(f\"{agent.name} final answer: {final_answer}\\n\")\n",
    "\n",
    "    # Calculate consensus\n",
    "    consensus_score = calculate_consensus(final_responses)\n",
    "    print(f\"Consensus Score: {consensus_score}\")\n",
    "\n",
    "def calculate_consensus(responses):\n",
    "    \"\"\"\n",
    "    Calculates a simple consensus score based on the uniqueness of responses.\n",
    "    \"\"\"\n",
    "    processed_responses = [response.strip().lower() for response in responses]\n",
    "    unique_responses = set(processed_responses)\n",
    "    consensus_score = 1.0 / len(unique_responses)\n",
    "    return consensus_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    agents = [\n",
    "        Agent(\n",
    "        name='Agent1',\n",
    "        role='mathematical reasoning',\n",
    "        model_name='meta-llama/Llama-3.1-8B-Instruct'\n",
    "        ),\n",
    "        Agent(\n",
    "            name='Agent2',\n",
    "            role='logical consistency',\n",
    "            model_name='Qwen/Qwen2.5-Coder-7B-Instruct'\n",
    "        ),\n",
    "    ]\n",
    "    question = \"What is the sum of all even numbers between 1 and 100?\"\n",
    "    run_debate(agents, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52670e-1595-4efc-8852-911aa4bd82cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22177f-749c-4165-8e1a-deb96077f13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
