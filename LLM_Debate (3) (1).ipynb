{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f6ddc4-59f9-4161-a1bd-5707c0a5d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0' torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36bc7d1-d82b-4195-a1c6-bc080a99e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, role, model_name):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=100,  # Limit the response length\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        self.history = []\n",
    "        self.score = 1.0\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "        prompt += \"\\nAssistant:\"\n",
    "        response = self.pipeline(prompt, max_new_tokens=100)[0]['generated_text']\n",
    "        response = response[len(prompt):].strip().split('Human:')[0].strip()\n",
    "        return response\n",
    "\n",
    "    def update_score(self, correct):\n",
    "        if correct:\n",
    "            self.score += 1.0\n",
    "        else:\n",
    "            self.score = max(0.1, self.score - 0.5)  # Prevent negative weights\n",
    "\n",
    "def construct_message(agents, idx, question, round_num, final=False, self_reflection=False):\n",
    "    messages = []\n",
    "    current_agent = agents[idx]\n",
    "    \n",
    "    # Add system message\n",
    "    messages.append({\n",
    "        'role': 'system',\n",
    "        'content': f\"You are Agent {current_agent.name}, an AI assistant specialized in {current_agent.role}. \"\n",
    "                   f\"Participate in a collaborative problem-solving discussion to answer the given question.\"\n",
    "    })\n",
    "    \n",
    "    # Add question\n",
    "    messages.append({'role': 'user', 'content': f\"Question: {question}\"})\n",
    "    \n",
    "    # Add previous rounds of discussion\n",
    "    for i, entry in enumerate(current_agent.history):\n",
    "        messages.append({'role': 'assistant', 'content': f\"Round {i + 1}: {entry['response']}\"})\n",
    "    \n",
    "    # Construct the prompt based on the current situation\n",
    "    if self_reflection:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Reflect on your previous answer and the responses of other agents. \"\n",
    "            f\"Can you improve or correct your previous answer? If so, how? If not, why not?\"\n",
    "        )\n",
    "    elif final:\n",
    "        prompt = (\n",
    "            f\"Based on all discussions, what is your final answer to the question: {question}? \"\n",
    "            f\"Provide detailed reasoning.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Consider the previous discussions and provide your perspective \"\n",
    "            f\"on the question: {question}. Focus on your specialization in {current_agent.role}.\"\n",
    "        )\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def run_debate(agents, question, max_rounds=3):\n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        print(f\"\\n=== Round {round_num} ===\")\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agents, agent.name, question, round_num)\n",
    "            response = agent.generate_response(messages)\n",
    "            agent.history.append({'round': round_num, 'response': response})\n",
    "            print(f\"{agent.name} ({agent.role}): {response}\\n\")\n",
    "        \n",
    "        # Self-reflection\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agents, agent.name, question, round_num, self_reflection=True)\n",
    "            reflection = agent.generate_response(messages)\n",
    "            print(f\"{agent.name} reflection: {reflection}\\n\")\n",
    "        \n",
    "    # Final answers\n",
    "    print(\"\\n=== Final Answers ===\")\n",
    "    for agent in agents:\n",
    "        messages = construct_message(agents, agent.name, question, max_rounds, final=True)\n",
    "        final_answer = agent.generate_response(messages)\n",
    "        print(f\"{agent.name} final answer: {final_answer}\\n\")\n",
    "\n",
    "def calculate_consensus(responses):\n",
    "    # Simple consensus calculation based on response similarity\n",
    "    processed_responses = [response.strip().lower() for response in responses]\n",
    "    unique_responses = set(processed_responses)\n",
    "    consensus_score = 1.0 / len(unique_responses)\n",
    "    return consensus_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653e8e84-a1a3-4170-a272-1ab8dc0f7921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3160b155e6e24ac49e2954fdd4fd5bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6892d3d2-9285-48d0-8148-d64ee54e6fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1 (mathematical reasoning): Thank you for the opportunity to tackle this problem. As an AI specializing in mathematical reasoning, I'll break it down into a manageable step-by-step approach.\n",
      "\n",
      "**Step 1: Identify the range of even numbers**\n",
      "The even numbers between 1 and 100 are 2, 4, 6,..., 98, 100.\n",
      "\n",
      "**Step 2: Determine the number of terms**\n",
      "To find the number of terms, we can use the formula for the nth term of an arithmetic sequence\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 (logical consistency): Sure, let's go through this together. \n",
      "\n",
      "Firstly, we need to find all the even numbers between 1 and 100. An even number is any integer that is divisible by 2 without leaving a remainder. The even numbers in this range are: 2, 4, 6,..., 100.\n",
      "\n",
      "Now, let's calculate their sum:\n",
      "\n",
      "1. The first even number is 2.\n",
      "2. The last even number is 100.\n",
      "\n",
      "Agent1 reflection: Thank you for the opportunity to reflect. Upon reviewing our previous discussion, I realized that my initial response was overly complex. To correct my previous answer, I'll provide a more straightforward solution.\n",
      "\n",
      "**Step 1: Identify the range of even numbers**\n",
      "The even numbers between 1 and 100 are 2, 4, 6,..., 98, 100.\n",
      "\n",
      "**Step 2: Determine the number of terms**\n",
      "Since the sequence starts at 2 and ends at 100,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 reflection: I apologize for the confusion earlier. Let's work through the problem step by step. The even numbers between 1 and 100 are: 2, 4, 6,..., 100. This is an arithmetic sequence where the first term (a1) is 2 and the common difference (d) is 2. The last term (an) is 100.\n",
      "\n",
      "To find the number of terms (n) in the sequence, we can use\n",
      "\n",
      "\n",
      "=== Round 2 ===\n",
      "Agent1 (mathematical reasoning): Thank you for the feedback. Based on our previous discussions, I'll re-evaluate the problem from a different angle.\n",
      "\n",
      "**Step 3: Find the common difference (d)**\n",
      "The common difference of an arithmetic sequence is the difference between any two consecutive terms. In this case, it's 2 (e.g., 2 - 4 = -2, but since we're counting even numbers, we'll use the positive difference: 4 - 2 = 2).\n",
      "\n",
      "**Step\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 (logical consistency): As an AI assistant specializing in logical consistency, my response is based on the assumption that we are discussing the sum of all even numbers between 1 and 100. \n",
      "\n",
      "To find the sum, we can use the formula for the sum of an arithmetic series. The formula is:\n",
      "\n",
      "Sum = n * (first term + last term) / 2\n",
      "\n",
      "where n is the number of terms, first term, and last term are the two numbers in the series.\n",
      "\n",
      "In this case, the\n",
      "\n",
      "Agent1 reflection: Round 3: I reflect on my previous answer and realize that I used an incorrect approach. The correct approach would be to use the formula for the sum of an arithmetic series, which is:\n",
      "\\[ S_n = \\frac{n}{2} (a_1 + a_n) \\]\n",
      "where \\( S_n \\) is the sum of the first n terms, \\( a_1 \\) is the first term, and \\( a_n \\) is the nth term.\n",
      "\n",
      "In this case,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 reflection: As an AI assistant, I have updated my previous response. The sum of all even numbers between 1 and 100 is 1000.\n",
      "\n",
      "\n",
      "=== Round 3 ===\n",
      "Agent1 (mathematical reasoning): Round 4: I'll dive deeper into the problem. To find the sum of all even numbers between 1 and 100, we can use the formula for the sum of an arithmetic series.\n",
      "\n",
      "**Step 1: Identify the first and last terms**\n",
      "The first even number between 1 and 100 is 2, and the last even number is 100.\n",
      "\n",
      "**Step 2: Identify the common difference**\n",
      "As we discussed earlier, the common difference is 2.\n",
      "\n",
      "**Step\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 (logical consistency): I am glad to assist you in understanding the problem and the question. Let's start with the first part of the question: What is the sum of all even numbers between 1 and 100?\n",
      "\n",
      "The even numbers between 1 and 100 are 2, 4, 6,..., 100. This is an arithmetic sequence where the first term (a) is 2, the common difference (d) is 2, and the last term (\n",
      "\n",
      "Agent1 reflection: Round 5: Upon reflection, I realize that I made an error in my previous response. I didn't properly identify the common difference and didn't use the correct formula for the sum of an arithmetic series. Let me correct my previous answer.\n",
      "\n",
      "**Step 1: Identify the range of even numbers**\n",
      "The even numbers between 1 and 100 are 2, 4, 6,..., 98, 100.\n",
      "\n",
      "**Step 2: Determine the number of terms**\n",
      "To find\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2 reflection: Yes, I can improve my previous answer by adding a few more details. Here's my revised response:\n",
      "\n",
      "The sum of all even numbers between 1 and 100 is 5050.\n",
      "\n",
      "The sum of all even numbers between 1 and 100 is 5050. This is because the sequence of even numbers is an arithmetic progression with the first term (a) as 2, the common difference (d) as 2, and the last term\n",
      "\n",
      "\n",
      "=== Final Answers ===\n",
      "Agent1 final answer: Round 5: Thank you for guiding me through this problem. Based on our collective efforts, I'll provide a comprehensive solution.\n",
      "\n",
      "Let's analyze the problem step by step:\n",
      "\n",
      "* The first even number is 2, and the last even number is 100.\n",
      "* The common difference is 2.\n",
      "* To find the number of terms, we can use the formula for the nth term of an arithmetic sequence:\n",
      "\n",
      "a_n = a_1 + (n - 1)d\n",
      "\n",
      "where a_n\n",
      "\n",
      "Agent2 final answer: The sum of all even numbers between 1 and 100 is 5050.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, role, model_name):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=1024,  # Limit the response length\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        self.history = []\n",
    "        self.score = 1.0\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "        prompt += \"\\nAssistant:\"\n",
    "        response = self.pipeline(prompt, max_new_tokens=100)[0]['generated_text']\n",
    "        response = response[len(prompt):].strip().split('Human:')[0].strip()\n",
    "        return response\n",
    "\n",
    "    def update_score(self, correct):\n",
    "        if correct:\n",
    "            self.score += 1.0\n",
    "        else:\n",
    "            self.score = max(0.1, self.score - 0.5)  # Prevent negative weights\n",
    "\n",
    "def construct_message(agent, question, round_num, final=False, self_reflection=False):\n",
    "    messages = []\n",
    "    \n",
    "    # Add system message\n",
    "    messages.append({\n",
    "        'role': 'system',\n",
    "        'content': f\"You are Agent {agent.name}, an AI assistant specialized in {agent.role}. \"\n",
    "                   f\"Participate in a collaborative problem-solving discussion to answer the given question.\"\n",
    "    })\n",
    "    \n",
    "    # Add question\n",
    "    messages.append({'role': 'user', 'content': f\"Question: {question}\"})\n",
    "    \n",
    "    # Add previous rounds of discussion\n",
    "    for i, entry in enumerate(agent.history):\n",
    "        messages.append({'role': 'assistant', 'content': f\"Round {i + 1}: {entry['response']}\"})\n",
    "    \n",
    "    # Construct the prompt based on the current situation\n",
    "    if self_reflection:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Reflect on your previous answer and the responses of other agents. \"\n",
    "            f\"Can you improve or correct your previous answer? If so, how? If not, why not?\"\n",
    "        )\n",
    "    elif final:\n",
    "        prompt = (\n",
    "            f\"Based on all discussions, what is your final answer to the question: {question}? \"\n",
    "            f\"Provide detailed reasoning.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Consider the previous discussions and provide your perspective \"\n",
    "            f\"on the question: {question}. Focus on your specialization in {agent.role}.\"\n",
    "        )\n",
    "    \n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def run_debate(agents, question, max_rounds=3):\n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        print(f\"\\n=== Round {round_num} ===\")\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num)\n",
    "            response = agent.generate_response(messages)\n",
    "            agent.history.append({'round': round_num, 'response': response})\n",
    "            print(f\"{agent.name} ({agent.role}): {response}\\n\")\n",
    "        \n",
    "        # Self-reflection\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num, self_reflection=True)\n",
    "            reflection = agent.generate_response(messages)\n",
    "            print(f\"{agent.name} reflection: {reflection}\\n\")\n",
    "        \n",
    "    # Final answers\n",
    "    print(\"\\n=== Final Answers ===\")\n",
    "    for agent in agents:\n",
    "        messages = construct_message(agent, question, max_rounds, final=True)\n",
    "        final_answer = agent.generate_response(messages)\n",
    "        print(f\"{agent.name} final answer: {final_answer}\\n\")\n",
    "\n",
    "def calculate_consensus(responses):\n",
    "    # Simple consensus calculation based on response similarity\n",
    "    processed_responses = [response.strip().lower() for response in responses]\n",
    "    unique_responses = set(processed_responses)\n",
    "    consensus_score = 1.0 / len(unique_responses)\n",
    "    return consensus_score\n",
    "\n",
    "# Usage\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "agents = [\n",
    "    Agent(\n",
    "        name='Agent1',\n",
    "        role='mathematical reasoning',\n",
    "        model_name='meta-llama/Llama-3.2-1b-Instruct'\n",
    "    ),\n",
    "    Agent(\n",
    "        name='Agent2',\n",
    "        role='logical consistency',\n",
    "        model_name='Qwen/Qwen2.5-0.5B'\n",
    "    ),\n",
    "    # Agent(\n",
    "    #     name='Agent3',\n",
    "    #     role='context integration',\n",
    "    #     model_name='deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct'\n",
    "    # ),\n",
    "]\n",
    "question = \"What is the sum of all even numbers between 1 and 100?\"\n",
    "run_debate(agents, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370ea6c-4a91-4327-a7c7-9daca403e697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21a505dbbcc4356b05ab9289f1ff927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, role, model_name):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32\n",
    "            ).to(self.device)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=self.device,\n",
    "            max_new_tokens=500,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        self.history = []\n",
    "        self.score = 1.0\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        prompt = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in messages])\n",
    "        prompt += \"\\nAssistant:\"\n",
    "        response = self.pipeline(prompt)[0]['generated_text']\n",
    "        # Extract the assistant's response\n",
    "        response = response[len(prompt):].strip().split('\\n')[0].strip()\n",
    "        return response\n",
    "\n",
    "    def update_score(self, correct):\n",
    "        if correct:\n",
    "            self.score += 1.0\n",
    "        else:\n",
    "            self.score = max(0.1, self.score - 0.5)\n",
    "\n",
    "def construct_message(agent, question, round_num, final=False, self_reflection=False):\n",
    "    messages = []\n",
    "\n",
    "    # Add system message\n",
    "    messages.append({\n",
    "        'role': 'system',\n",
    "        'content': f\"You are {agent.name}, an AI assistant specialized in {agent.role}. \"\n",
    "                   f\"Participate in a collaborative problem-solving discussion to answer the given question.\"\n",
    "    })\n",
    "\n",
    "    # Add question\n",
    "    messages.append({'role': 'user', 'content': f\"Question: {question}\"})\n",
    "\n",
    "    # Add previous rounds of discussion\n",
    "    for i, entry in enumerate(agent.history):\n",
    "        messages.append({'role': 'assistant', 'content': f\"Round {entry['round']}: {entry['response']}\"})\n",
    "\n",
    "    # Construct the prompt based on the current situation\n",
    "    if self_reflection:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Reflect on your previous answer and the responses of other agents. \"\n",
    "            f\"Can you improve or correct your previous answer? If so, how? If not, why not?\"\n",
    "        )\n",
    "    elif final:\n",
    "        prompt = (\n",
    "            f\"Based on all discussions, what is your final answer to the question: {question}? \"\n",
    "            f\"Provide detailed reasoning.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"This is round {round_num}. Consider the previous discussions and provide your perspective \"\n",
    "            f\"on the question: {question}. Focus on your specialization in {agent.role}.\"\n",
    "        )\n",
    "\n",
    "    messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "    return messages\n",
    "\n",
    "def run_debate(agents, question, max_rounds=3):\n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        print(f\"\\n=== Round {round_num} ===\")\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num)\n",
    "            response = agent.generate_response(messages)\n",
    "            agent.history.append({'round': round_num, 'response': response})\n",
    "            print(f\"{agent.name} ({agent.role}): {response}\\n\")\n",
    "\n",
    "        # Self-reflection\n",
    "        for agent in agents:\n",
    "            messages = construct_message(agent, question, round_num, self_reflection=True)\n",
    "            reflection = agent.generate_response(messages)\n",
    "            print(f\"{agent.name} reflection: {reflection}\\n\")\n",
    "\n",
    "    # Final answers\n",
    "    print(\"\\n=== Final Answers ===\")\n",
    "    final_responses = []\n",
    "    for agent in agents:\n",
    "        messages = construct_message(agent, question, max_rounds, final=True)\n",
    "        final_answer = agent.generate_response(messages)\n",
    "        final_responses.append(final_answer)\n",
    "        print(f\"{agent.name} final answer: {final_answer}\\n\")\n",
    "\n",
    "    # Calculate consensus\n",
    "    consensus_score = calculate_consensus(final_responses)\n",
    "    print(f\"Consensus Score: {consensus_score}\")\n",
    "\n",
    "def calculate_consensus(responses):\n",
    "    \"\"\"\n",
    "    Calculates a simple consensus score based on the uniqueness of responses.\n",
    "    \"\"\"\n",
    "    processed_responses = [response.strip().lower() for response in responses]\n",
    "    unique_responses = set(processed_responses)\n",
    "    consensus_score = 1.0 / len(unique_responses)\n",
    "    return consensus_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    agents = [\n",
    "        Agent(\n",
    "        name='Agent1',\n",
    "        role='mathematical reasoning',\n",
    "        model_name='meta-llama/Llama-3.1-8B-Instruct'\n",
    "        ),\n",
    "        Agent(\n",
    "            name='Agent2',\n",
    "            role='logical consistency',\n",
    "            model_name='Qwen/Qwen2.5-1.5B-Instruct'\n",
    "        ),\n",
    "    ]\n",
    "    question = \"What is the sum of all even numbers between 1 and 100?\"\n",
    "    run_debate(agents, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52670e-1595-4efc-8852-911aa4bd82cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22177f-749c-4165-8e1a-deb96077f13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f51ff-d1bf-4594-9e7e-3a48540698be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423532e8-0902-4ec7-89f6-32c1fa93773a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492d11d-db13-4143-870e-8bcd60588dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69416f12-6ccc-4d81-a672-413fc8e28009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fef415-880c-4b73-97fd-c3dc3fd3febd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83d70d-c2d7-4738-a7b8-a83961c488d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
